# qwen-lora-windows-guide generated by chatgpt
ğŸ§  Train Your Own LoRA on Qwen 0.6B in Under a Day (Windows, 8GB+ VRAM)

This is a beginner-friendly Windows guide to fine-tune Qwen 0.6B with LoRA using the Alpaca dataset. Works on consumer GPUs like RTX 2060, 3060, or 3080 Ti.

ğŸ“ Step 1: Download the Base Model

Go to: (https://huggingface.co/Qwen/Qwen3-0.6B-Base/tree/main)

Download the files manually or use huggingface's download manager, place them in a folder named Qwen3-0.6B-Base. Remember its location.

ğŸ“¦ Step 2: Install Dependencies (Windows)

Make sure you're using Python 3.10 or 3.11 (Python 3.12 is not fully compatible).

Run these in Command Prompt or PowerShell:

py -m pip install torch --index-url https://download.pytorch.org/whl/cu121

py -m pip install transformers datasets accelerate bitsandbytes einops scipy peft

ğŸ“š Step 3: Download the Alpaca Dataset

Save the JSON file from:
https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json


ğŸ”¬ Step 4: Fine-Tuning Script (trainllmlora.py)

Download trainllmlora.py (in this repo - see link above) and update the file paths to match your setup.

Make sure the script loads:

The base Qwen model

The Alpaca dataset

Your chosen output directory for saving the LoRA adapter

ğŸš€ Step 5: Run the Training

Open your terminal or VS Code and run:

python trainllmlora.py

ğŸ§° Step 6: Inference Script (runllmlora.py)

Download runllmlora.py(in this repo - see link above) , modify paths in it to yours.

python run_qwen_lora.py

ğŸ¤” Final Notes

Reduce VRAM usage by lowering max_length and batch sizes, try smaller quant values in the bitsnbytes config

You can use your own dataset in Alpaca format
