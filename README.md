# qwen-lora-windows-guide generated by chatgpt
üß† Train Your Own LoRA on Qwen 0.6B in Under a Day (Windows, 8GB+ VRAM)

This is a beginner-friendly Windows guide to fine-tune Qwen 0.6B with LoRA using the Alpaca dataset. Works on consumer GPUs like RTX 2060, 3060, or 3080 Ti.

üìÅ Step 1: Download the Base Model

Go to: https://huggingface.co/Qwen/Qwen-0.6B-Base

Download these files manually:

config.json

generation_config.json

model.safetensors or pytorch_model.bin

tokenizer_config.json

tokenizer.model

special_tokens_map.json

Place them in:

D:\AI\models\Qwen-0.6B-Base

(Or any path of your choice)

üì¶ Step 2: Install Dependencies (Windows)

Make sure you're using Python 3.10 or 3.11 (Python 3.12 is not fully compatible).

Run these in Command Prompt or PowerShell:

pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install transformers datasets accelerate bitsandbytes einops scipy peft

If you hit an error about bitsandbytes on Windows, skip it or try this Windows port workaround.

üìö Step 3: Download the Alpaca Dataset

Save the JSON file from:
https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json

Place it in:

D:\AI\data\alpaca_data.json

üî¨ Step 4: Fine-Tuning Script (fine_tune_qwen.py)

Download the file trainllmlora.py and modify dir paths to your downloads.

üöÄ Step 5: Run the Training

Open your terminal or VS Code and run:

python fine_tune_qwen.py

The adapter will be saved in:

D:\AI\qwen_lora_output

üß∞ Step 6: Inference Script (run_qwen_lora.py)

To test your LoRA model, create run_qwen_lora.py:

from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from peft import PeftModel
import torch

base_model = "D:/AI/models/Qwen-0.6B-Base"
lora_model = "D:/AI/qwen_lora_output"

tokenizer = AutoTokenizer.from_pretrained(base_model, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    load_in_4bit=True,
    device_map="auto",
    torch_dtype=torch.float16,
    local_files_only=True
)
model = PeftModel.from_pretrained(model, lora_model)

prompt = "### Instruction:\nTell me a joke about programmers.\n\n### Input:\n\n### Response:\n"

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generation_config = GenerationConfig(
    max_new_tokens=100,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

with torch.no_grad():
    output = model.generate(**inputs, generation_config=generation_config)
    print(tokenizer.decode(output[0], skip_special_tokens=True))

Then run:

python run_qwen_lora.py

ü§î Final Notes

Reduce VRAM usage by lowering max_length and batch sizes

You can use your own dataset in Alpaca format

For 4-bit quantization to work, make sure bitsandbytes installs cleanly (or skip for full precision)

Optional: You can convert to GGUF for use in llama.cpp if you want CPU inference.

Let me know if you want a one-click zip package or GUI wrapper!

