# qwen-lora-windows-guide generated by chatgpt
ğŸ§  Train Your Own LoRA on Qwen 0.6B in Under a Day (Windows, 8GB+ VRAM)

This is a beginner-friendly Windows guide to fine-tune Qwen 0.6B with LoRA using the Alpaca dataset. Works on consumer GPUs like RTX 2060, 3060, or 3080 Ti.

ğŸ“ Step 1: Download the Base Model

Go to: (https://huggingface.co/Qwen/Qwen3-0.6B-Base/tree/main)

Download the files manually or use huggingface's download manager.


(Or any path of your choice)

ğŸ“¦ Step 2: Install Dependencies (Windows)

Make sure you're using Python 3.10 or 3.11 (Python 3.12 is not fully compatible).

Run these in Command Prompt or PowerShell:

pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install transformers datasets accelerate bitsandbytes einops scipy peft

If you hit an error about bitsandbytes on Windows, skip it or try this Windows port workaround.

ğŸ“š Step 3: Download the Alpaca Dataset

Save the JSON file from:
https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json


ğŸ”¬ Step 4: Fine-Tuning Script (trainllmlora.py)

Download the file trainllmlora.py and modify dir paths to your downloads.

ğŸš€ Step 5: Run the Training

Open your terminal or VS Code and run:

python trainllmlora.py

ğŸ§° Step 6: Inference Script (runllmlora.py)

Download runllmlora.py, modify dirs in it to yours.

python run_qwen_lora.py

ğŸ¤” Final Notes

Reduce VRAM usage by lowering max_length and batch sizes, try smaller quant values in the bitsnbytes config

You can use your own dataset in Alpaca format
